{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Applied\n",
    "\n",
    "Agenda:\n",
    "- Review code for Voting Classifier, Bagging Classifier, and Random Forest\n",
    "- Practice finding optimal hyperparameter for  Random Forest with gridsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Prep Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import BaggingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-ds-033020-lectures/master/Mod_3/decision_trees/cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use x and y variables to split the training data into train and test set then scale that data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7975460122699386\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_preds = knn.predict(X_test)\n",
    "\n",
    "knn_f1 = metrics.f1_score(y_test, knn_preds)\n",
    "\n",
    "\n",
    "print(knn_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Logistic Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8066298342541436\n"
     ]
    }
   ],
   "source": [
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "lr_f1 = metrics.f1_score(y_test, lr_preds)\n",
    "\n",
    "print(lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8047337278106509\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=5, class_weight='balanced')\n",
    "\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "dtc_preds  = dtc.predict(X_test)\n",
    "\n",
    "dtc_f1 = metrics.f1_score(y_test, dtc_preds)\n",
    "\n",
    "print(dtc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine three models using Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the estimators, we must provide a list of tuples. The first value in the tuple is is the name given to the model/estimator in the second value. SKlearn requires this because there is additional functionality where you can access information about the specific models, so you need to name the models to access them later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8160919540229884\n"
     ]
    }
   ],
   "source": [
    "# ENSEMBLE BELOW\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr), ('knneighbors', knn), ('decisiontree', dtc)], \n",
    "                voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "vc_preds = voting_clf.predict(X_test)\n",
    "\n",
    "vc_f1 = metrics.f1_score(y_test, vc_preds)\n",
    "\n",
    "print(vc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a voting classifier with multiple Logistic regression models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C_param_range = [0.001,0.01,0.1,0.5,1] #gneralization\n",
    "titles = ['lr_0_001', 'lr_0_01', 'lr_0_1', 'lr0_5', 'lr_1']\n",
    "\n",
    "params = dict(zip(titles, C_param_range)) #create a dictionary\n",
    "models = {}\n",
    "\n",
    "table = pd.DataFrame(columns = ['C_parameter','F1']) #create a table\n",
    "table['C_parameter'] = C_param_range\n",
    "j = 0\n",
    "\n",
    "for k , v  in params.items(): #can change the value of c \n",
    "    \n",
    "    # Create model using different value for c  \n",
    "    lr = LogisticRegression(penalty = 'l2', C = v, random_state = 1, class_weight='balanced')\n",
    "    \n",
    "    #save the model to a dictionary to use later in our voting classifiers\n",
    "    models[k]= lr\n",
    "    \n",
    "    #the steps below this point are unnecessary in order to create a voting classifier, \n",
    "    #but it is easy to fit the model and see how performance changes for different levels of regularization\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_preds = lr.predict(X_test)\n",
    "\n",
    "    # Saving accuracy score in table\n",
    "    table.iloc[j,1] = metrics.f1_score(y_test, y_preds)\n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_parameter</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.735135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.751381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.804469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.80663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.80663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_parameter        F1\n",
       "0        0.001  0.735135\n",
       "1        0.010  0.751381\n",
       "2        0.100  0.804469\n",
       "3        0.500   0.80663\n",
       "4        1.000   0.80663"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review performance for different levels of C\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lr_0_001',\n",
       "  LogisticRegression(C=0.001, class_weight='balanced', random_state=1)),\n",
       " ('lr_0_01',\n",
       "  LogisticRegression(C=0.01, class_weight='balanced', random_state=1)),\n",
       " ('lr_0_1',\n",
       "  LogisticRegression(C=0.1, class_weight='balanced', random_state=1)),\n",
       " ('lr0_5', LogisticRegression(C=0.5, class_weight='balanced', random_state=1)),\n",
       " ('lr_1', LogisticRegression(C=1, class_weight='balanced', random_state=1))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invesitgate the models D=dictionary\n",
    "list(models.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have programmatically created multiple logistic regression models, let's use them in an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "lr_voting = VotingClassifier(estimators=list(models.items()), \n",
    "                              voting='hard')\n",
    "\n",
    "lr_voting.fit(X_train, y_train)\n",
    "\n",
    "lrv_preds = lr_voting.predict(X_test)\n",
    "\n",
    "lrv_f1 = metrics.f1_score(y_test, lrv_preds)\n",
    "\n",
    "print(lrv_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still learning from the same data so it doesn't do much better, they aren't as diverse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Bagging Classifier for a Logistic Regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666, 9)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_lr = BaggingClassifier(\n",
    "            base_estimator=LogisticRegression(random_state = 1, class_weight='balanced'), \n",
    "            n_estimators= 100, #100 logistic regression models\n",
    "            max_samples= 0.8, #sample with replacement (bootstrap)\n",
    "            max_features= 6,\n",
    "            oob_score= True #am looking for this pertaining to the best estimator of the model\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc_knn = BaggingClassifier(\n",
    "#             base_estimator=KNeighborsClassifierh(n_neighbors=9), \n",
    "#             n_estimators= 100, #100 logistic regression models\n",
    "#             max_samples= 0.8, #sample with replacement (bootstrap)\n",
    "#             max_features= 6,\n",
    "#             oob_score= True #am looking for this pertaining to the best estimator of the model\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc_dtc = BaggingClassifier(\n",
    "#             base_estimator=DecisionTree(max_depth=5, class_weight='balanced'), \n",
    "#             n_estimators= 100, #100 logistic regression models\n",
    "#             max_samples= 0.8, #sample with replacement (bootstrap)\n",
    "#             max_features= 6,\n",
    "#             oob_score= True #am looking for this pertaining to the best estimator of the model\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_clf_bagged = VotingClassifier(\n",
    "#                 estimators=[('logreg', bc_lr), ('knneighbors', bc_knn), ('decisiontree', dc_dtc)], \n",
    "#                 voting='hard')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_clf_bagged.fit(X_train, y_train)\n",
    "\n",
    "# vc_bagged_preds = voting_clf_bagged.predict(X_test)\n",
    "\n",
    "# vc_bagged_f1 = metrics.f1_score(y_test, vc_bagged_preds)\n",
    "\n",
    "# print(vc_bagged_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(class_weight='balanced',\n",
       "                                                    random_state=1),\n",
       "                  max_features=6, max_samples=0.8, n_estimators=100,\n",
       "                  oob_score=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_lr.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7792792792792793"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the oob_score to get some idea of how the model performs on a validation set\n",
    "\n",
    "bc_lr.oob_score_ #accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "# See how the model performs on the test set\n",
    "\n",
    "bc_lr_preds = bc_lr.predict(X_test)\n",
    "\n",
    "bc_lr_f1 = metrics.f1_score(y_test, bc_lr_preds)\n",
    "\n",
    "print(bc_lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the difference in the `VotingClassifier` algorithm and the `BaggingClassifier` algorithm?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer: voting classifier is a hard vote vs bagging that is a soft vote \n",
    "\n",
    "samples and features are consistent through all models\n",
    "\n",
    "randomly selects features and samples with the same penalties throughout the model for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between a BaggingClassifier that uses a decision tree as the base estimator and a Random Forest Classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest classifier will take a sample of features at each node, where as a bagging classifier will take a sample of features at to use for the whole model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state = 1, n_estimators=1000, max_depth=2, max_features=4) \n",
    "#max_depth=1 - decision  - not neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, max_features=4, n_estimators=1000,\n",
       "                       random_state=1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at all the different default features\n",
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, max_features=4, n_estimators=1000,\n",
       "                       random_state=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model to the training data\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score:  0.7074829931972789\n"
     ]
    }
   ],
   "source": [
    "#use the fitted model to predict on the test data\n",
    "rfc_preds = rfc.predict(X_test)\n",
    "\n",
    "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test F1 score: ', rfc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Increase the number of trees and see how the model performs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridsearchCV with Random Forest\n",
    "\n",
    "Let's use grid search to identify the best tuning parameters to use for a random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of all the parameters you want to tune - multiple options\n",
    "param_grid = { \n",
    "    'n_estimators': [300,500,700],\n",
    "    'max_leaf_nodes' : range(10,20,2),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(2,10)),\n",
    "    'max_features': ['auto', 5,6, None, .08]\n",
    "}\n",
    "\n",
    "#reducing range of the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a grid search object and fit it to the data\n",
    "\n",
    "grid_tree=GridSearchCV(RandomForestClassifier(), param_grid, cv=10, scoring='f1', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7549283543204516\n",
      "{'criterion': 'entropy', 'max_depth': 9, 'max_features': 5, 'n_estimators': 500}\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=9, max_features=5,\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "### Identify the best params \n",
    "\n",
    "\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)\n",
    "#Identify the best score during fitting with cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8024691358024691\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test)\n",
    "\n",
    "# Model F1, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
